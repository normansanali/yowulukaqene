<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=description content="We have already taken a look at the various iSCSI options available in DSM 5.0 for virtualization-ready NAS units. This section presents the benchmarks for various types of iSCSI LUNs on the ioSafe 1513+. It is divided into three parts, one dealing with our benchmarking setup, the second providing the actual performance numbers and the"><meta name=author content="Larita Shotwell"><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/base16/css/style.css type=text/css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type=text/css><link rel=alternate href=./index.xml type=application/rss+xml title=JBlogX><title>DSM 5.0: Evaluating iSCSI Performance - JBlogX</title></head><body><header><div class="container clearfix"><a class=path href=./index.html>[JBlogX]</a>
<span class=caret># _</span><div class=right></div></div></header><div class=container><main role=main class=article><article class=single itemscope itemtype=http://schema.org/BlogPosting><div class=meta><span class=key>published on</span>
<span class=val><time itemprop=datePublished datetime=2024-09-23>September 23, 2024</time></span>
<span class=key>in</span>
<span class=val><a href=./categories/blog>blog</a></span></div><h1 class=headline itemprop=headline>DSM 5.0: Evaluating iSCSI Performance</h1><section class=body itemprop=articleBody><p>We have already taken a look at the various iSCSI options available in DSM 5.0 for virtualization-ready NAS units. This section presents the benchmarks for various types of iSCSI LUNs on the ioSafe 1513+. It is divided into three parts, one dealing with our benchmarking setup, the second providing the actual performance numbers and the final one providing some notes on our experience with the iSCSI features as well as some analysis of the numbers.</p><h3>Benchmark Setup</h3><p>Hardware-wise, the NAS testbed used for multi-client CIFS evaluation was utilized here too. The Windows Server 2008 R2 + Hyper-V setup can run up to 25 Windows 7 virtual machines concurrently. The four LAN ports of the ioSafe 1513+ were bonded together in LACP mode (802.3ad link aggregation) for a 4 Gbps link. Jumbo frame settings were left at default (1500 bytes) and all the LUN / target configurations were left at default too (unless explicitly noted here).</p><p>Synology provides three different ways to create iSCSI LUNs, and we benchmarked each of them separately. For the file-based LUNs configuration, we created 25 different LUNs and mapped them on to 25 different targets. Each of the 25 VMs in our testbed connected to one target/LUN combination. The standard <a href=#>IOMeter benchmarks</a> that we use for multi-client CIFS evaluation were utilized for iSCSI evaluation also. The main difference to note is that the CIFS evaluation was performed on a mounted network share, while the iSCSI evaluation was done on a 'clean physical disk' (from the viewpoint of the virtual machine). A similar scheme was used for the block-level Multiple LUNs on RAID configuration also.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/8366/multi-init_575px.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>For the Single LUN on RAID configuration, we had only one target/LUN combination. Synology has an option to enable multiple initiators to map an iSCSI target (for cluster-aware operating systems), and we enabled that. This allowed the same target to map on to all the 25 VMs in our testbed. For this LUN configuration alone, the IOMeter benchmark scripts were slightly modified to change the starting sector on the 'physical disk' for each machine. This allowed each VM to have its own allocated space on which the IOMeter traces could be played out.</p><h3>Performance Numbers</h3><p>The four IOMeter traces were run on the physical disk manifested by mapping the iSCSI target on each VM. The benchmarking started with one VM accessing the NAS. The number of VMs simultaneously playing out the trace was incremented one by one till we had all 25 VMs in the fray. Detailed listings of the IOMeter benchmark numbers (including IOPS and maximum response times) for each configuration are linked below:</p><p align=center><img alt="ioSafe 1513+ - iSCSI LUN (Regular Files) Multi-Client iSCSI Performance - 100% Sequential Reads" id=image0 src=https://cdn.statically.io/img/images.anandtech.com/reviews/nas/iscsi/iosafe_1513p/graphs/iosafe_1513p_iscsi_lun_regular_files_100p_readseq.png width=600 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p align=center></p><p align=center>&nbsp;</p><p align=center><img alt="ioSafe 1513+ - iSCSI LUN (Regular Files) Multi-Client iSCSI Performance - Max Throughput - 50% Reads" id=image1 src=https://cdn.statically.io/img/images.anandtech.com/reviews/nas/iscsi/iosafe_1513p/graphs/iosafe_1513p_iscsi_lun_regular_files_mt_50p_reads.png width=600 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p align=center></p><p align=center>&nbsp;</p><p align=center><img alt="ioSafe 1513+ - iSCSI LUN (Regular Files) Multi-Client iSCSI Performance - Random 8K - 70% Reads" id=image2 src=https://cdn.statically.io/img/images.anandtech.com/reviews/nas/iscsi/iosafe_1513p/graphs/iosafe_1513p_iscsi_lun_regular_files_rand8k_70p_reads.png width=600 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p align=center></p><p align=center>&nbsp;</p><p align=center><img alt="ioSafe 1513+ - iSCSI LUN (Regular Files) Multi-Client iSCSI Performance - Real Life - 65% Reads" id=image3 src=https://cdn.statically.io/img/images.anandtech.com/reviews/nas/iscsi/iosafe_1513p/graphs/iosafe_1513p_iscsi_lun_regular_files_rl_65p_reads.png width=600 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p align=center></p><h3>Analysis</h3><p>Synology's claims of 'Single LUN on RAID' providing the best access performance holds true for large sequential reads. In other access patterns, the regular file-based LUNs perform quite well.. However, the surprising aspect is that none of the configurations can actually saturate the network links to the extent that the multi-client CIFS accesses did. In fact, the best number that we saw (in the Single LUN on RAID) case was around 220 MBps compared to the 300+ MBps that we obtained in our CIFS benchmarks.</p><p>The more worrisome fact was that our unit completely locked up while processing the 25-client regular file-based LUNs benchmark routine. On the VMs' side, we found that the target simply couldn't be accessed. The NAS itself was unresponsive to access over SSH or HTTP. Pressing the front power button resulted in a blinking blue light, but the unit wouldn't shut down. There was no alternative, but to yank out the power cord in order to shut down the unit. By default, the Powershell script for iSCSI benchmarking starts with one active VM, processes the IOMeter traces, adds one more VM to the mix and repeats the process - this is done in a loop till the script reaches a stage where all the 25 VMs are active and have run the four IOMeter traces. After restarting the ioSafe 1513+, we reran the Powershell script by enabling the 25-client access alone and the benchmark completed without any problems. Strangely, this issue happened only for the file-based LUNs, and the two sets of block-based iSCSI LUN benchmarks completed without any problems. I searched online and found at least one other person <a href=#>reporting</a> a similar issue, albeit, with a more complicated setup using MPIO (multi-path I/O) - a feature we didn't test out here.</p><p>Vendors in this market space usually offer only file-based LUNs to tick the iSCSI marketing checkbox. Some vendors reserve block-level LUNs only for their high-end models. So, Synology must be appreciated for bringing block-based LUNs as an available feature to almost all its products. In our limited evaluation, we found that stability could improve for file-based LUNs. Performance could also do with some improvement, considering that a 4 Gbps aggregated link could not be saturated. With a maximum of around 220 MBps, it is difficult to see how a LUN store on the ioSafe / Synology 1513+ could withstand a 'VM boot storm' (a situation where a large number of virtual machines using LUNs on the same NAS as the boot disk try to start up simultaneously). That said, the unit should be able to handle two or three such VMs / LUNs quite easily.</p><p>From our coverage perspective, we talked about Synology DSM's iSCSI feature because it is one of the more comprehensive offerings in this market space. If readers are interested, we can process our multi-VM iSCSI for other SMB-targeted NAS units too. It may reveal details of where each vendor stands when it comes to supporting virtualization scenarios. Feel free to sound off in the comments.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZIV0gpVooKirkZuybn2UampmqpWrtqbDjJpknaGjlsC1sdGrnKyho6mur8CMrLCnp5yktLp5w6xobmljZIQ%3D</p></section></article></main></div><footer><div class=container><span class=copyright>&copy; 2024 JBlogX - <a rel=license href=http://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></span></div></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>